# -*- coding: utf-8 -*-
"""Rag-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HNuc0VrPND_0NWidyF54kfAjFVNx08Dh
"""

import pandas as pd
import os
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Path to your file
file_path = 'collection_jobs.csv'

# Load the CSV with '|' as the delimiter and handle bad lines by skipping them
try:
    data = pd.read_csv(file_path, delimiter='|', on_bad_lines='skip')
    print("Data loaded successfully.")
except Exception as e:
    print("Error loading CSV file:", e)

# Data cleaning steps
data = data.dropna()
data = data.drop_duplicates()
data = data.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

# Save cleaned data
cleaned_file_path = 'cleaned_collection_jobs.csv'
data.to_csv(cleaned_file_path, index=False)
print(f"Cleaned data has been saved to '{cleaned_file_path}'.")

# Load cleaned dataset
try:
    df = pd.read_csv(cleaned_file_path)
    print("Cleaned data loaded successfully.")
except Exception as e:
    print("Error loading cleaned CSV file:", e)

# Ensure 'description' column is preprocessed
if 'description' in df.columns:
    df['description'] = df['description'].fillna('')
else:
    print("'description' column is missing from the DataFrame.")

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Initialize the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for job descriptions
embeddings = model.encode(df['description'].tolist())

# Create FAISS index
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(np.array(embeddings))
print(f"Indexed {index.ntotal} descriptions.")

def retrieve_jobs(query, model, index, top_n=5):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_n)
    results = df.iloc[indices[0]].copy()
    results['distance'] = distances[0]
    return results[['title', 'description', 'distance']]

# Test with a sample query
query = "nurse"
results = retrieve_jobs(query, model, index)
print("Top job matches for query:")
print(results)

# Use Ollama's model
try:
    import ollama
    model_name = "llama2"
    test_prompt = "give jobs in schmalkalden. Do not include any metadata or extra information."
    test_response = ollama.generate(model=model_name, prompt=test_prompt)
    
    test_generated_text = "Test Summary:\n"
    
    if isinstance(test_response, dict) and 'response' in test_response:
        test_generated_text += test_response['response'].strip() + "\n"
    else:
        print("No valid response received from Ollama.")
    
    print(test_generated_text)

except Exception as e:
    print("An error occurred while using Ollama:", e)
