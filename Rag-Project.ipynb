{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6e0b765-edd9-45fb-a269-346ed41aa946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Cleaned data has been saved to 'cleaned_collection_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "file_path = 'collection_jobs.csv'\n",
    "\n",
    "# Load the CSV with '|' as the delimiter and handle bad lines by skipping them\n",
    "try:\n",
    "    data = pd.read_csv(file_path, delimiter='|', on_bad_lines='skip')\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Data cleaning steps (example):\n",
    "    # 1. Drop rows with missing values (if needed)\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "\n",
    "    # 3. Additional cleaning steps based on column requirements\n",
    "    # For example, trimming whitespace from string columns\n",
    "    data = data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "    # Saving the cleaned data to a new file for full viewing\n",
    "    cleaned_file_path = 'cleaned_collection_jobs.csv'\n",
    "    data.to_csv(cleaned_file_path, index=False)\n",
    "    print(f\"Cleaned data has been saved to '{cleaned_file_path}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12381679-6151-47f8-948d-1e1c7a538e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "file_path = 'cleaned_collection_jobs.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'job_description' column is preprocessed\n",
    "df['description'] = df['description'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43267b59-7aac-4d2a-835e-f364b13bc1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.2.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.46.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!pip install sentence-transformers faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "580331e2-36e7-4c87-84c5-329e53410ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for job descriptions\n",
    "embeddings = model.encode(df['description'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59506080-12c9-4725-a2c9-b05de5186cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2278 descriptions.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to a format suitable for FAISS\n",
    "embedding_dim = embeddings.shape[1]  # Typically 384 for 'all-MiniLM-L6-v2'\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # Use L2 distance for similarity\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(np.array(embeddings))\n",
    "print(f\"Indexed {index.ntotal} descriptions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "972b077a-0fcf-4f11-b84c-5a02ae888be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_jobs(query, model, index, top_n=5):\n",
    "    # Convert query to embedding\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = index.search(np.array(query_embedding), top_n)\n",
    "    \n",
    "    # Fetch the results\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    results['distance'] = distances[0]\n",
    "    return results[['title', 'description', 'distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b68146e-5e76-4f8f-b076-7fb8afd51f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top job matches for query:\n",
      "                                                 title  \\\n",
      "735            Specialist nurse (m/f/d) skilled worker   \n",
      "760   Certified nursing specialist (m/f/d) immediately   \n",
      "905                                 Nurse (m/f/d) care   \n",
      "2166                                 Nursing assistant   \n",
      "2165     Nursing specialist on night duty, night watch   \n",
      "\n",
      "                                            description  distance  \n",
      "735   Are you looking for a professional reorientati...  1.025185  \n",
      "760   From craftsmen to production and warehouse emp...  1.025958  \n",
      "905   Are you looking for a professional reorientati...  1.044787  \n",
      "2166  The focus of our work at the AWO is always the...  1.108999  \n",
      "2165  The focus of our work at AWO is always people,...  1.118136  \n"
     ]
    }
   ],
   "source": [
    "# Test with a sample query\n",
    "query = \"nurse\"\n",
    "results = retrieve_jobs(query, model, index)\n",
    "\n",
    "print(\"Top job matches for query:\")\n",
    "print(results)\n",
    "\n",
    "\n",
    "import ollama\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf21a60a-1d26-45c4-87ef-7823a468950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from Ollama for test prompt:\n",
      "Type of test_response: <class 'dict'>\n",
      "Test Summary:\n",
      "1. Software Developer\n",
      "2. Mechanical Engineer\n",
      "3. Marketing Manager\n",
      "4. Sales Representative\n",
      "5. IT Consultant\n",
      "6. Operations Manager\n",
      "7. Human Resources Manager\n",
      "8. Graphic Designer\n",
      "9. Web Developer\n",
      "10. Accountant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "model_name = \"llama2\"  # or any model name you're using\n",
    "\n",
    "\n",
    "# Simple prompt to test text generation capability with clearer instructions\n",
    "test_prompt = \"give jobs in thuringia. Do not include any metadata or extra information.\"\n",
    "\n",
    "# Use Ollama's `generate` function to query the model.\n",
    "test_response = ollama.generate(model=model_name, prompt=test_prompt)\n",
    "\n",
    "# Initialize output text\n",
    "test_generated_text = \"Test Summary:\\n\"  # Adding a label to the generated text\n",
    "\n",
    "# Collecting and filtering the test response\n",
    "try:\n",
    "    print(\"Raw response from Ollama for test prompt:\")\n",
    "    print(f\"Type of test_response: {type(test_response)}\")  # Check the type of the response\n",
    "\n",
    "    if isinstance(test_response, dict):\n",
    "        # Check if 'response' key exists in the dictionary\n",
    "        if 'response' in test_response:\n",
    "            test_generated_text += test_response['response'].strip() + \"\\n\"\n",
    "        else:\n",
    "            print(\"No 'response' key found in the dictionary.\")\n",
    "    else:\n",
    "        print(\"Unexpected response format. Please check the model output.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while generating test text: {e}\")\n",
    "\n",
    "# Output the test generated text\n",
    "print(test_generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2425fc-3080-4b2b-bc40-a45cde334ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9277560-14fa-4594-b293-fe7c5c80760d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e305d9-fc3f-4c66-92b0-6758c864d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
